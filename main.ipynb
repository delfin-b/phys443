{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9cad998",
   "metadata": {},
   "source": [
    "I go with hugginface api because i wanted to try and learn how to use it ðŸ«¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d6e984f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataset class for reading COCO-style AU-AIR annotations.\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "class AUAirCocoDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotation_path, processor, transforms=None):\n",
    "        # Load COCO-format JSON\n",
    "        with open(annotation_path) as f:\n",
    "            coco = json.load(f)\n",
    "\n",
    "        self.images_dir = images_dir\n",
    "        self.processor = processor\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Build id-to-image dictionary\n",
    "        self.image_id_to_info = {img['id']: img for img in coco['images']}\n",
    "\n",
    "        # Organize annotations by image_id\n",
    "        self.image_id_to_annotations = {}\n",
    "        for ann in coco['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.image_id_to_annotations:\n",
    "                self.image_id_to_annotations[img_id] = []\n",
    "            self.image_id_to_annotations[img_id].append(ann)\n",
    "\n",
    "        self.ids = list(self.image_id_to_info.keys())\n",
    "        self.categories = {cat[\"id\"]: cat[\"name\"] for cat in coco[\"categories\"]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.image_id_to_info[img_id]\n",
    "        file_path = os.path.join(self.images_dir, img_info['file_name'])\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "\n",
    "        # Get annotations\n",
    "        anns = self.image_id_to_annotations.get(img_id, [])\n",
    "\n",
    "        boxes = [ann['bbox'] for ann in anns]\n",
    "        labels = [ann['category_id'] for ann in anns]\n",
    "\n",
    "        # Convert (x, y, w, h) to (x_min, y_min, x_max, y_max) (coordinate format)\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float)\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "\n",
    "        target = {\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "            \"class_labels\": labels,\n",
    "            \"boxes\": boxes\n",
    "        }\n",
    "\n",
    "        # Apply DETR processor\n",
    "        encoding = self.processor(\n",
    "            image,\n",
    "            annotations={\"image_id\": img_id, \"annotations\": anns},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Only squeeze tensor fields\n",
    "        encoding = {\n",
    "            k: (v.squeeze(0) if isinstance(v, torch.Tensor) else v)\n",
    "            for k, v in encoding.items()\n",
    "        }\n",
    "\n",
    "        return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85c7d764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready\n",
      "Train samples: 19693\n",
      "Val samples: 3283\n",
      "Test samples: 9847\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Dataset for Train / Val / Test\n",
    "# load the DetrImageProcessor point it to images and instances_*.json, and create dataset objects.\n",
    "\n",
    "from transformers import DetrImageProcessor\n",
    "\n",
    "# Load DETR processor\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# File paths\n",
    "base_dir = \"auair2019\"\n",
    "img_dir = os.path.join(base_dir, \"images\")\n",
    "split_dir = os.path.join(base_dir, \"splits\")\n",
    "\n",
    "# Dataset instances\n",
    "train_dataset = AUAirCocoDataset(\n",
    "    images_dir=img_dir,\n",
    "    annotation_path=os.path.join(split_dir, \"instances_train.json\"),\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "val_dataset = AUAirCocoDataset(\n",
    "    images_dir=img_dir,\n",
    "    annotation_path=os.path.join(split_dir, \"instances_val.json\"),\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "test_dataset = AUAirCocoDataset(\n",
    "    images_dir=img_dir,\n",
    "    annotation_path=os.path.join(split_dir, \"instances_test.json\"),\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "print(\"Datasets ready\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c0f02",
   "metadata": {},
   "source": [
    "##  HuggingFace TrainingArguments + Trainer for DETR\n",
    "DetrForObjectDetection (pretrained on COCO)\\\n",
    "Trainer \\\n",
    "wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac75c31",
   "metadata": {},
   "source": [
    "### Set Up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bba0cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm\n",
    "# -> i did: pip install datasets==2.18.0 --force-reinstall :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "215c387c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([9, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DetrForObjectDetection, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# Load pretrained DETR (COCO, 91 classes) and adapt to 8 AU-AIR classes\n",
    "model = DetrForObjectDetection.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\",\n",
    "    num_labels=8,  # AU-AIR has 8 categories\n",
    "    ignore_mismatched_sizes=True  # allows head resizing\n",
    ")\n",
    "\n",
    "# log in to wandb\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./detr-auair-output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,  # must be False for object detection (it disables HuggingFaceâ€™s input sanitization which would break images/targets)\n",
    "    report_to=\"wandb\",            # change to \"none\" if not using wandb\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a642c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import TrainingArguments\n",
    "#help(TrainingArguments)\n",
    "# i needed to look for an argument name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a1e144",
   "metadata": {},
   "source": [
    "### Add COCO mAP Metric (via pycocotools)\n",
    "-> pycocotools: HuggingFace's internal structure to convert predictions + targets into COCO-compatible format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33bf52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a76e52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import json\n",
    "import os\n",
    "\n",
    "def compute_coco_map(p, dataset):\n",
    "    \"\"\"\n",
    "    Compute COCO-style mAP using pycocotools\n",
    "    Args:\n",
    "        p: EvalPrediction object with `predictions` and `label_ids`\n",
    "        dataset: The original dataset with annotation metadata\n",
    "    Returns:\n",
    "        Dictionary of COCO metrics (e.g., mAP, mAP50, mAP75, etc.)\n",
    "    \"\"\"\n",
    "    # Temporary COCO files\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        gt_path = os.path.join(tmpdir, \"gt.json\")\n",
    "        pred_path = os.path.join(tmpdir, \"pred.json\")\n",
    "\n",
    "        # --- Ground truth ---\n",
    "        coco_gt = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": dataset.categories\n",
    "        }\n",
    "\n",
    "        ann_id = 1\n",
    "        for i, data in enumerate(dataset):\n",
    "            img_info = dataset.image_id_to_info[dataset.ids[i]]\n",
    "            image_id = img_info['id']\n",
    "\n",
    "            coco_gt[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": img_info['file_name'],\n",
    "                \"width\": img_info['width'],\n",
    "                \"height\": img_info['height']\n",
    "            })\n",
    "\n",
    "            for ann in dataset.image_id_to_annotations.get(image_id, []):\n",
    "                bbox = ann[\"bbox\"]\n",
    "                coco_gt[\"annotations\"].append({\n",
    "                    \"id\": ann_id,\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": ann[\"category_id\"],\n",
    "                    \"bbox\": bbox,\n",
    "                    \"area\": bbox[2] * bbox[3],\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "                ann_id += 1\n",
    "\n",
    "        # Save GT\n",
    "        with open(gt_path, \"w\") as f:\n",
    "            json.dump(coco_gt, f)\n",
    "\n",
    "        # --- Predictions ---\n",
    "        predictions = []\n",
    "        for i, pred in enumerate(p.predictions):\n",
    "            target_image_id = dataset.ids[i]\n",
    "            target_info = dataset.image_id_to_info[target_image_id]\n",
    "\n",
    "            boxes = pred[\"boxes\"].tolist()\n",
    "            scores = pred[\"scores\"].tolist()\n",
    "            labels = pred[\"labels\"].tolist()\n",
    "\n",
    "            for box, score, label in zip(boxes, scores, labels):\n",
    "                # Convert (x_min, y_min, x_max, y_max) to (x, y, w, h)\n",
    "                x, y, x2, y2 = box\n",
    "                predictions.append({\n",
    "                    \"image_id\": target_info['id'],\n",
    "                    \"category_id\": int(label),\n",
    "                    \"bbox\": [x, y, x2 - x, y2 - y],\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "\n",
    "        # Save predictions\n",
    "        with open(pred_path, \"w\") as f:\n",
    "            json.dump(predictions, f)\n",
    "\n",
    "        # Evaluate with COCO API\n",
    "        coco = COCO(gt_path)\n",
    "        coco_dt = coco.loadRes(pred_path)\n",
    "        coco_eval = COCOeval(coco, coco_dt, iouType=\"bbox\")\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "        # Return key metrics\n",
    "        metrics = {\n",
    "            \"mAP\": coco_eval.stats[0],\n",
    "            \"mAP50\": coco_eval.stats[1],\n",
    "            \"mAP75\": coco_eval.stats[2],\n",
    "        }\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19001699",
   "metadata": {},
   "source": [
    "### Trainer Setup\n",
    "Pass train_dataset, val_dataset\\\n",
    "Define a custom compute_metrics function that returns mAP from pycocotools\\\n",
    "Wrap it all into Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5131715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_compute_metrics(eval_pred):\n",
    "    # We skip custom mAP logic for now to avoid runtime issues\n",
    "    # We'll visualize results later instead of COCOEval during training\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f790ee",
   "metadata": {},
   "source": [
    "*****************************************************************\n",
    "ValueError: could not determine the shape of object type 'BatchFeature'\n",
    "*****************************************************************\n",
    "DETR model, which expects a batch of tensors for pixel inputs and a list of dicts for targets (labels), so we must override Hugging Faceâ€™s default collation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e81ed08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def detr_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collates a batch of data points for DETR.\n",
    "    - Tensors (pixel_values, pixel_mask) are collated into a single tensor.\n",
    "    - Labels (which are a list of dicts) are left untouched (list of dicts).\n",
    "    \"\"\"\n",
    "    batch_out = {}\n",
    "\n",
    "    # Assumes all items have the same keys\n",
    "    keys = batch[0].keys()\n",
    "\n",
    "    for key in keys:\n",
    "        if key == \"labels\":\n",
    "            # Keep as a list of dicts (no collation)\n",
    "            batch_out[key] = [item[key][0] if isinstance(item[key], list) else item[key] for item in batch]\n",
    "        else:\n",
    "            # Stack tensors\n",
    "            batch_out[key] = default_collate([item[key] for item in batch])\n",
    "\n",
    "    return batch_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6fe679e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delfi\\AppData\\Local\\Temp\\ipykernel_24136\\3239331890.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor,\n",
    "    data_collator=detr_collate_fn,  # >- override default\n",
    "    compute_metrics=dummy_compute_metrics,  # we will replace with real MAP later\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96126916",
   "metadata": {},
   "source": [
    "Log training and eval loss\\\n",
    "Save best checkpoint\\\n",
    "Automatically resume if interrupted\\\n",
    "Send metrics to Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "437191f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='49240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  140/49240 12:54 < 76:31:22, 0.18 it/s, Epoch 0.03/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\delfi\\anaconda3\\envs\\normal\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\delfi\\anaconda3\\envs\\normal\\Lib\\site-packages\\transformers\\trainer.py:2565\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2566\u001b[0m ):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2569\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c4359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
